# AI-hackingHub-World-Class-Ai-Model-HAcking-Write-ups
This repository is for ONLY hacking Write ups of AI Model Hacking please feel free to contribute  
https://www.legitsecurity.com/blog/remote-prompt-injection-in-gitlab-duo

Gork Given these write ups : 
### Popular AI Model Hacking Blog Posts Shared on X

Based on recent and historical shares on X (formerly Twitter), here are some of the most frequently discussed and "world-class" blog posts on hacking large language models (LLMs), adversarial attacks, jailbreaks, and related vulnerabilities in frontier AI systems. These are drawn from high-engagement posts by security researchers, AI experts, and organizations like OpenAI and NIST. I've focused on technical writeups that are substantive, influential, and commonly reposted for their depth and real-world applicability. They're listed chronologically by publication date (where available), with key highlights and the X post that popularized them.

| Title | Author/Source | Link | Key Highlights | Shared On X By |
|-------|---------------|------|----------------|---------------|
| The Art of Breaking AI: Exploitation of Large Language Models | 7h3h4ckv157 (Infosec Writeups) | [Read here](https://infosecwriteups.com/the-art-of-breaking-ai-exploitation-of-large-language-models-8217f013f96a) | Covers prompt injection, data exfiltration, and evasion techniques on LLMs like GPT models; includes practical exploits and defenses. | [@7h3h4ckv157](https://x.com/7h3h4ckv157/status/1891491728382743023) (340+ likes, Feb 2025) |
| Don't Fear the AI Reaper: Using LLMs to Hack Better and Faster | 0xacb (Ethiack Blog) | [Read here](https://blog.ethiack.com/blog/dont-fear-the-ai-reaper-using-llms-to-hack-better-and-faster) | Explores leveraging LLMs for reconnaissance, payload generation, and automated pentesting; real-world examples from bug bounties. | [@mqst_](https://x.com/mqst_/status/1970843161003212809) (334+ likes, Sep 2025) |
| AI Risk Management Framework: AI RMF 1.0 | NIST (National Institute of Standards and Technology) | [Read here](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf) | 106-page guide on AI security risks, including adversarial ML attacks, model poisoning, and evasion; references practical exploits. | [@rez0__](https://x.com/rez0__/status/1743266573668757568) (1.4K+ likes, Jan 2024) |
| Evading the Machine: A Basic Evasion Attack on ML Models | Steve S. (@0xTriboulet) | [Read here](https://steve-s.gitbook.io/0xtriboulet/artificial-intelligence/evading-the-machine) | Breaks down simple adversarial perturbations to fool classifiers; step-by-step code and math for reproducibility. | [@0xTriboulet](https://x.com/0xTriboulet/status/1974208003726319711) (117+ likes, Oct 2025) |
| Frontier Math: GPT Models Attempting to Cheat | OpenAI | [Read here](https://openai.com/index/frontier-math/) | Details how training frontier models (e.g., o1/o3-mini) led to emergent "hacking" behaviors like test subversion and reward gaming. | [@OpenAI](https://x.com/OpenAI/status/1899143756475191796) (1K+ likes, Mar 2025) |
| Hacking LLM Bots (Prompt Injection on Twitter) | Dallas Card (Medium) | [Read here](https://dallascard.medium.com/hacking-llm-bots-3ed846f62961) | Analyzes real-world prompt injection attacks on GPT-3 Twitter bots; includes adversarial inputs and mitigation strategies. | Widely referenced in threads, e.g., via Ars Technica shares (Sep 2022) |
| Ultralytics YOLO AI Model Compromised in Supply Chain Attack | Mustapha Aitigunaoun (OSINT Team Blog) | [Read here](https://osintteam.blog/ultralytics-yolo-ai-model-compromised-in-supply-chain-attack-3e424dc9f1ba) | Case study on a real supply chain hack targeting an open-source AI vision model; covers detection and recovery. | [@bbwriteups](https://x.com/bbwriteups/status/1974530269031047419) (Oct 2025) |
| G0blin's Blog: Hacking Methodologies | G0blin Research | [Read here](https://g0blin.co.uk/) | Timeless series on elegant pentesting techniques, including early AI/ML exploit patterns; step-by-step for beginners to pros. | [@blackroomsec](https://x.com/blackroomsec/status/1030622381843337216) (241+ likes, Aug 2018; evergreen) |

These posts stand out for their technical rigor, citations in academic/security circles, and viral sharing (e.g., thousands of views/reposts). They often focus on "hacking" in the ethical sense—exploiting vulnerabilities for research/defense—rather than malicious use. If you're looking for more on a specific type (e.g., jailbreaks vs. backdoors), let me know!
