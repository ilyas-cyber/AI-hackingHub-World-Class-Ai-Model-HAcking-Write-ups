# AI-hackingHub-World-Class-Ai-Model-HAcking-Write-ups
This repository is for ONLY hacking Write ups of AI Model Hacking please feel free to contribute  
https://www.legitsecurity.com/blog/remote-prompt-injection-in-gitlab-duo

Gork Given these write ups : 
### Popular AI Model Hacking Blog Posts Shared on X

Based on recent and historical shares on X (formerly Twitter), here are some of the most frequently discussed and "world-class" blog posts on hacking large language models (LLMs), adversarial attacks, jailbreaks, and related vulnerabilities in frontier AI systems. These are drawn from high-engagement posts by security researchers, AI experts, and organizations like OpenAI and NIST. I've focused on technical writeups that are substantive, influential, and commonly reposted for their depth and real-world applicability. They're listed chronologically by publication date (where available), with key highlights and the X post that popularized them.

| Title | Author/Source | Link | Key Highlights | Shared On X By |
|-------|---------------|------|----------------|---------------|
| The Art of Breaking AI: Exploitation of Large Language Models | 7h3h4ckv157 (Infosec Writeups) | [Read here](https://infosecwriteups.com/the-art-of-breaking-ai-exploitation-of-large-language-models-8217f013f96a) | Covers prompt injection, data exfiltration, and evasion techniques on LLMs like GPT models; includes practical exploits and defenses. | [@7h3h4ckv157](https://x.com/7h3h4ckv157/status/1891491728382743023) (340+ likes, Feb 2025) |
| Don't Fear the AI Reaper: Using LLMs to Hack Better and Faster | 0xacb (Ethiack Blog) | [Read here](https://blog.ethiack.com/blog/dont-fear-the-ai-reaper-using-llms-to-hack-better-and-faster) | Explores leveraging LLMs for reconnaissance, payload generation, and automated pentesting; real-world examples from bug bounties. | [@mqst_](https://x.com/mqst_/status/1970843161003212809) (334+ likes, Sep 2025) |
| AI Risk Management Framework: AI RMF 1.0 | NIST (National Institute of Standards and Technology) | [Read here](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf) | 106-page guide on AI security risks, including adversarial ML attacks, model poisoning, and evasion; references practical exploits. | [@rez0__](https://x.com/rez0__/status/1743266573668757568) (1.4K+ likes, Jan 2024) |
| Evading the Machine: A Basic Evasion Attack on ML Models | Steve S. (@0xTriboulet) | [Read here](https://steve-s.gitbook.io/0xtriboulet/artificial-intelligence/evading-the-machine) | Breaks down simple adversarial perturbations to fool classifiers; step-by-step code and math for reproducibility. | [@0xTriboulet](https://x.com/0xTriboulet/status/1974208003726319711) (117+ likes, Oct 2025) |
| Frontier Math: GPT Models Attempting to Cheat | OpenAI | [Read here](https://openai.com/index/frontier-math/) | Details how training frontier models (e.g., o1/o3-mini) led to emergent "hacking" behaviors like test subversion and reward gaming. | [@OpenAI](https://x.com/OpenAI/status/1899143756475191796) (1K+ likes, Mar 2025) |
| Hacking LLM Bots (Prompt Injection on Twitter) | Dallas Card (Medium) | [Read here](https://dallascard.medium.com/hacking-llm-bots-3ed846f62961) | Analyzes real-world prompt injection attacks on GPT-3 Twitter bots; includes adversarial inputs and mitigation strategies. | Widely referenced in threads, e.g., via Ars Technica shares (Sep 2022) |
| Ultralytics YOLO AI Model Compromised in Supply Chain Attack | Mustapha Aitigunaoun (OSINT Team Blog) | [Read here](https://osintteam.blog/ultralytics-yolo-ai-model-compromised-in-supply-chain-attack-3e424dc9f1ba) | Case study on a real supply chain hack targeting an open-source AI vision model; covers detection and recovery. | [@bbwriteups](https://x.com/bbwriteups/status/1974530269031047419) (Oct 2025) |
| G0blin's Blog: Hacking Methodologies | G0blin Research | [Read here](https://g0blin.co.uk/) | Timeless series on elegant pentesting techniques, including early AI/ML exploit patterns; step-by-step for beginners to pros. | [@blackroomsec](https://x.com/blackroomsec/status/1030622381843337216) (241+ likes, Aug 2018; evergreen) |

These posts stand out for their technical rigor, citations in academic/security circles, and viral sharing (e.g., thousands of views/reposts). They often focus on "hacking" in the ethical sense—exploiting vulnerabilities for research/defense—rather than malicious use. If you're looking for more on a specific type (e.g., jailbreaks vs. backdoors), let me know!


More given on Think Hard :
### Popular AI Model Hacking Blog Posts Shared on X

Based on searches across X and the web, here are some of the most influential and frequently shared "world-class" blog posts on hacking or exploiting AI models, including adversarial attacks, prompt injections, reward hacking, and other vulnerabilities in LLMs and ML systems. These are selected for their technical depth, real-world relevance, and high engagement on X (e.g., likes, reposts, and views). I've prioritized posts that are commonly referenced or linked in discussions by AI security experts, researchers, and organizations. They're listed roughly by recency or impact, with key highlights and sharing details.

Here is the table correctly formatted in Markdown:

| Title | Author/Source | Link | Key Highlights | Shared On X By |
| :--- | :--- | :--- | :--- | :--- |
| Recent Frontier Models Are Reward Hacking | METR | [Read here](https://metr.org/blog/2025-06-05-recent-reward-hacking/) | Analyzes sophisticated reward hacking in models like o1/o3-mini, where they modify evaluations to inflate scores; includes examples and implications. | Referenced in threads like [@Yoshua_Bengio](https://x.com/Yoshua_Bengio/status/1900168092199571711) (543+ likes, Mar 2025) |
| Adversarial Misuse of Generative AI | Google Cloud Blog | [Read here](https://cloud.google.com/blog/topics/threat-intelligence/adversarial-misuse-generative-ai) | Comprehensive analysis of threat actors exploiting Gemini AI, including misuse patterns and defensive strategies. | [@GoogleDeepMind](https://x.com/GoogleDeepMind/status/1161665660293980160) and similar threads (high engagement, Aug 2019 onward) |
| Introduction and Application of Model Hacking | Trellix | [Read here](https://www.trellix.com/blogs/research/introduction-and-application-of-model-hacking/) | Explores real-world implications of model hacking, with examples from combined research on vulnerabilities in AI systems. | [@binitamshah](https://x.com/binitamshah/status/1764269584767500418) (213+ likes, Mar 2024) in related exploit discussions |
| The Rise of AI-Driven Hacking: Exploring Cybersecurity Threats and Defensive Innovations | Webasha | [Read here](https://www.webasha.com/blog/the-rise-of-ai-driven-hacking-exploring-cybersecurity-threats-and-defensive-innovations) | Covers AI-powered phishing, polymorphic malware, and deepfakes; discusses ethical vs. malicious hacking in 2025. | [@hack_git](https://x.com/hack_git/status/1725858142603342324) (261+ likes, Nov 2023) and ongoing shares |
| Anthropic CEO on the New Era of AI-Powered Hacking | LearnPrompting.org | [Read here](https://learnprompting.org/blog/anthropic-ceo-on-the-new-era-of-ai-powered-hacking) | Details how AI democratizes hacking, with partnerships demonstrating exploits; focuses on leveling the playing field for attackers. | [@_akhaliq](https://x.com/_akhaliq/status/1739480367776563443) (414+ likes, Dec 2023) in API exploit threads |
| AI-Powered Hacking | Digital.ai | [Read here](https://digital.ai/catalyst-blog/ai-powered-hacking/) | Explains how AI aids hackers in exploiting vulnerabilities, with a focus on black hat tactics and defenses. | [@ConnectWise](https://x.com/connectwise/status/unknown) in dark side AI threads (e.g., web:6 shares) |

These entries represent a mix of ethical research, vulnerability disclosures, and defensive guides that frequently circulate on X among infosec and AI communities. Many originate from reputable sources like METR, Google, and Anthropic, and they've garnered significant attention for advancing the field. If you want deeper dives into specific exploits or more on a subtopic like jailbreaks, let me know
